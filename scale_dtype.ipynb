{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "990f176a-891d-48a2-a6e0-9932a6aa6646",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hqq.core.quantize import (\n",
    "    BaseQuantizeConfig, \n",
    "    HQQLinear\n",
    ")\n",
    "from hqq.models.hf.base import AutoHQQHFModel\n",
    "from lm_eval.models.huggingface import HFLM\n",
    "\n",
    "import gc\n",
    "import lm_eval\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c42f83a-8b74-4111-836d-5cc568784196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize(model: torch.nn.Module, group_size: int, scale_dtype: torch.dtype) -> torch.nn.Module:\n",
    "    qconfig = BaseQuantizeConfig(\n",
    "        nbits = 4,\n",
    "        group_size = group_size,\n",
    "        axis = 1,\n",
    "        quant_zero = False,\n",
    "    )\n",
    "\n",
    "    # Use RTN quantization (no HQQ algorithm).\n",
    "    qconfig[\"weight_quant_params\"][\"optimize\"] = False\n",
    "\n",
    "    AutoHQQHFModel.quantize_model(model, quant_config=qconfig, compute_dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "    # Simulate low-precision scale dtypes by round-trip conversion\n",
    "    if scale_dtype != torch.float32:\n",
    "        for linear in filter(lambda m: isinstance(m, HQQLinear), model.modules()):\n",
    "            linear.meta['scale'] = linear.meta['scale'].to(scale_dtype).to(torch.float32)\n",
    "            linear.meta['zero'] = linear.meta['zero'].to(scale_dtype).to(torch.float32)\n",
    "\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c5c7e64-305e-4544-a070-09346a5cdddc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-25:01:50:40,617 INFO     [huggingface.py:162] Using device 'cuda'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing group_size=None, scale_dtype=None...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gregory/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f42744b445e4aab9325f8ab585aa56e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "2024-06-25:01:50:45,105 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "2024-06-25:01:50:48,135 WARNING  [task.py:763] [Task: wikitext] metric word_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity\n",
      "2024-06-25:01:50:48,136 WARNING  [task.py:775] [Task: wikitext] metric word_perplexity is defined, but higher_is_better is not. using default higher_is_better=False\n",
      "2024-06-25:01:50:48,137 WARNING  [task.py:763] [Task: wikitext] metric byte_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity\n",
      "2024-06-25:01:50:48,137 WARNING  [task.py:775] [Task: wikitext] metric byte_perplexity is defined, but higher_is_better is not. using default higher_is_better=False\n",
      "2024-06-25:01:50:48,137 WARNING  [task.py:763] [Task: wikitext] metric bits_per_byte is defined, but aggregation is not. using default aggregation=bits_per_byte\n",
      "2024-06-25:01:50:48,138 WARNING  [task.py:775] [Task: wikitext] metric bits_per_byte is defined, but higher_is_better is not. using default higher_is_better=False\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "2024-06-25:01:50:49,913 WARNING  [repocard.py:107] Repo card metadata block was not found. Setting CardData to empty.\n",
      "2024-06-25:01:50:49,934 WARNING  [evaluator.py:222] Overwriting default num_fewshot of wikitext from None to 0\n",
      "2024-06-25:01:50:49,936 INFO     [task.py:395] Building contexts for wikitext on rank 0...\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 62/62 [00:00<00:00, 1137.13it/s]\n",
      "2024-06-25:01:50:49,994 INFO     [evaluator.py:362] Running loglikelihood_rolling requests\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 62/62 [00:35<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'group_size': None, 'scale_dtype': None, 'word_perplexity': 10.807844752731176}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group_size</th>\n",
       "      <th>scale_dtype</th>\n",
       "      <th>word_perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10.807845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  group_size scale_dtype  word_perplexity\n",
       "0       None        None        10.807845"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_sizes = [None, 32, 64, 128, 256]\n",
    "scale_dtypes = [torch.float32, torch.float16, torch.bfloat16]\n",
    "\n",
    "result_rows = []\n",
    "\n",
    "for group_size in group_sizes:\n",
    "    for scale_dtype in scale_dtypes if group_size is not None else [None]:\n",
    "        print(f\"Testing group_size={group_size}, scale_dtype={scale_dtype}...\")\n",
    "        \n",
    "        hflm = HFLM(\n",
    "            # pretrained = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "            pretrained = \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "            device = \"cuda\",\n",
    "            max_length = 2048,\n",
    "        )\n",
    "\n",
    "        if group_size is not None and scale_dtype is not None:\n",
    "            quantize(hflm.model, group_size = group_size, scale_dtype = scale_dtype)\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        results = lm_eval.simple_evaluate(\n",
    "            model = hflm,\n",
    "            tasks = [\"wikitext\"],\n",
    "            num_fewshot = 0,\n",
    "        )['results']\n",
    "        \n",
    "        del hflm\n",
    "        \n",
    "        result_rows.append({\n",
    "            \"group_size\": group_size,\n",
    "            \"scale_dtype\": scale_dtype,\n",
    "            \"word_perplexity\": results['wikitext']['word_perplexity,none']\n",
    "        })\n",
    "\n",
    "        print(result_rows[-1])\n",
    "\n",
    "result_df = pd.DataFrame.from_records(result_rows)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cedd5eed-4ea0-4102-89d6-99477c6da525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">word_perplexity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_size</th>\n",
       "      <th>32</th>\n",
       "      <th>64</th>\n",
       "      <th>128</th>\n",
       "      <th>256</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scale_dtype</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>torch.float32</th>\n",
       "      <td>11.057797</td>\n",
       "      <td>11.121785</td>\n",
       "      <td>11.337771</td>\n",
       "      <td>11.342766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>torch.float16</th>\n",
       "      <td>11.058110</td>\n",
       "      <td>11.121726</td>\n",
       "      <td>11.337575</td>\n",
       "      <td>11.342822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>torch.bfloat16</th>\n",
       "      <td>11.054658</td>\n",
       "      <td>11.125918</td>\n",
       "      <td>11.338817</td>\n",
       "      <td>11.338682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               word_perplexity                                 \n",
       "group_size                 32         64         128        256\n",
       "scale_dtype                                                    \n",
       "torch.float32        11.057797  11.121785  11.337771  11.342766\n",
       "torch.float16        11.058110  11.121726  11.337575  11.342822\n",
       "torch.bfloat16       11.054658  11.125918  11.338817  11.338682"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = pd.DataFrame.from_records(result_rows)\n",
    "result_df.pivot_table(index='scale_dtype', columns='group_size', sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "423d4fb8-7a73-4fa2-924c-aea6e05eeae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group_size</th>\n",
       "      <th>scale_dtype</th>\n",
       "      <th>word_perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32</td>\n",
       "      <td>torch.float32</td>\n",
       "      <td>11.057797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>torch.float16</td>\n",
       "      <td>11.058110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "      <td>11.054658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64</td>\n",
       "      <td>torch.float32</td>\n",
       "      <td>11.121785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "      <td>torch.float16</td>\n",
       "      <td>11.121726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>64</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "      <td>11.125918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>128</td>\n",
       "      <td>torch.float32</td>\n",
       "      <td>11.337771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>128</td>\n",
       "      <td>torch.float16</td>\n",
       "      <td>11.337575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>128</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "      <td>11.338817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>256</td>\n",
       "      <td>torch.float32</td>\n",
       "      <td>11.342766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>256</td>\n",
       "      <td>torch.float16</td>\n",
       "      <td>11.342822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>256</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "      <td>11.338682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    group_size     scale_dtype  word_perplexity\n",
       "0           32   torch.float32        11.057797\n",
       "1           32   torch.float16        11.058110\n",
       "2           32  torch.bfloat16        11.054658\n",
       "3           64   torch.float32        11.121785\n",
       "4           64   torch.float16        11.121726\n",
       "5           64  torch.bfloat16        11.125918\n",
       "6          128   torch.float32        11.337771\n",
       "7          128   torch.float16        11.337575\n",
       "8          128  torch.bfloat16        11.338817\n",
       "9          256   torch.float32        11.342766\n",
       "10         256   torch.float16        11.342822\n",
       "11         256  torch.bfloat16        11.338682"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
