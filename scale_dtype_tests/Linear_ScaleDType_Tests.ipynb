{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1133992-c368-4dcf-8ea3-7fe5d8153b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "from torchao.quantization.quant_api import (\n",
    "    quantize_,\n",
    "    int8_dynamic_activation_int4_weight,\n",
    ")\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a590cf90-d633-4f61-af70-2119683ed69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from torchao - modified to allow scale_dtype != act_dtype\n",
    "from torchao.quantization.quant_primitives import (\n",
    "    choose_qparams_affine,\n",
    "    quantize_affine,\n",
    "    dequantize_affine,\n",
    "    ZeroPointDomain,\n",
    "    MappingType,\n",
    "    int_scaled_matmul,\n",
    "    quantize_affine_hqq,\n",
    "    FP8_TYPES,\n",
    "    choose_qparams_affine_fpx,\n",
    "    quantize_affine_fpx,\n",
    "    dequantize_affine_fpx,\n",
    ")\n",
    "\n",
    "def _int8_asymm_per_token_quant(x: torch.Tensor) -> torch.Tensor:\n",
    "    mapping_type = MappingType.ASYMMETRIC\n",
    "    target_dtype = torch.int8\n",
    "    return to_affine_quantized_intx(x, mapping_type, _get_per_token_block_size(x), target_dtype)\n",
    "\n",
    "def apply_int8_dynamic_activation_int4_weight_quant_custom(weight, group_size=32, scale_dtype=torch.float32):\n",
    "    if weight.shape[-1] % group_size != 0:\n",
    "        return weight\n",
    "\n",
    "    # weight settings\n",
    "    mapping_type = MappingType.SYMMETRIC\n",
    "    block_size = (1, group_size)\n",
    "    target_dtype = torch.int8\n",
    "    eps = torch.finfo(torch.float32).eps\n",
    "    quant_min = -8\n",
    "    quant_max = 7\n",
    "\n",
    "    # input settings\n",
    "    input_quant_func = _int8_asymm_per_token_quant\n",
    "\n",
    "    weight = to_affine_quantized_intx(weight, mapping_type, block_size, target_dtype, quant_min, quant_max, eps, scale_dtype=scale_dtype)\n",
    "    weight = to_linear_activation_quantized(weight, input_quant_func)\n",
    "    return weight\n",
    "\n",
    "def int8_dynamic_activation_int4_weight_custom(group_size=32, scale_dtype=torch.float32):\n",
    "    def insert_subclass(lin):\n",
    "        lin.weight = torch.nn.Parameter(apply_int8_dynamic_activation_int4_weight_quant_custom(lin.weight, group_size, scale_dtype), requires_grad=False)\n",
    "        return lin\n",
    "\n",
    "    return insert_subclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce3e94ab-1809-4103-b35e-1969ae7246f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quant_with_scale_dtype(model, scale_dtype, group_size):\n",
    "    qmodel = copy.deepcopy(model)\n",
    "    #quantize_(qmodel, int8_dynamic_activation_int4_weight_custom(group_size=group_size, scale_dtype=scale_dtype))\n",
    "    quantize_(qmodel, int8_dynamic_activation_int4_weight(group_size=group_size))\n",
    "    \n",
    "    if scale_dtype != torch.float32:\n",
    "        for name, m in qmodel.named_modules():\n",
    "            if isinstance(m, torch.nn.Linear):\n",
    "                original_weight_tensor = m.weight.data.original_weight_tensor\n",
    "                new_scales = original_weight_tensor.layout_tensor.scale.to(scale_dtype).to(torch.float32)\n",
    "                original_weight_tensor.layout_tensor.scale.copy_(new_scales)\n",
    "\n",
    "    return qmodel\n",
    "\n",
    "def record_activations(model, run_func):\n",
    "    with torch.no_grad():\n",
    "        recorded_activations = {}\n",
    "    \n",
    "        def hook(module, hook_in, hook_out, key):\n",
    "            hook_out = hook_out.to(\"cpu\")\n",
    "            if key in recorded_activations:\n",
    "                recorded_activations[key].append(hook_out)\n",
    "            else:\n",
    "                recorded_activations[key] = [hook_out]\n",
    "    \n",
    "        def make_hook(key):\n",
    "            return lambda m, i, o: hook(m, i, o, key)\n",
    "    \n",
    "        # Set hooks on all linear modules\n",
    "        hooks = []\n",
    "        for name, mod in model.named_modules():\n",
    "            if isinstance(mod, torch.nn.Linear):\n",
    "                hooks.append(mod.register_forward_hook(make_hook(name)))\n",
    "    \n",
    "        # Run forward pass\n",
    "        outputs = run_func(input)\n",
    "        print(outputs)\n",
    "    \n",
    "        # Clear hooks\n",
    "        for h in hooks:\n",
    "            h.remove()\n",
    "    \n",
    "        return recorded_activations\n",
    "\n",
    "def record_activations_by_dtype(model_id, group_size=32, max_new_tokens=1):\n",
    "    prompt = \"Once upon a time, \"\n",
    "    generation_kwargs = {\n",
    "        \"do_sample\": False,\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "    }\n",
    "\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"cuda\",\n",
    "        torch_dtype=torch.float32,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    base_pipe = pipeline(\"text-generation\", model=base_model, tokenizer=tokenizer)\n",
    "    base_activations = record_activations(base_pipe.model, lambda _: base_pipe(prompt, **generation_kwargs))\n",
    "\n",
    "    del base_model\n",
    "\n",
    "    results = {\n",
    "        \"base\": base_activations,\n",
    "    }\n",
    "\n",
    "    cases = [\n",
    "        (\"f32s\", torch.float32),\n",
    "        (\"f16s\", torch.float16),\n",
    "        (\"bf16s\", torch.bfloat16),\n",
    "    ]\n",
    "\n",
    "    for key, dtype in cases:\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            device_map=\"cuda\",\n",
    "            torch_dtype=torch.float32,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        \n",
    "        quant_model = quant_with_scale_dtype(base_model, dtype, group_size)\n",
    "        del base_model\n",
    "        gc.collect()\n",
    "        \n",
    "        quant_pipe = pipeline(\"text-generation\", model=quant_model, tokenizer=tokenizer)\n",
    "        quant_activations = record_activations(quant_model, lambda _: quant_pipe(prompt, **generation_kwargs))\n",
    "        results[key] = quant_activations\n",
    "        del quant_model\n",
    "        gc.collect()\n",
    "        \n",
    "\n",
    "    return results\n",
    "\n",
    "def compare_activations(base_acts, f32s_acts, quant_acts):\n",
    "    err1 = f32s_acts - base_acts\n",
    "    signed_err1 = err1.abs()\n",
    "\n",
    "    err2 = quant_acts - base_acts\n",
    "    signed_err2 = err2.abs()\n",
    "    \n",
    "    return [\n",
    "        (signed_err1.mean(), signed_err1.max(), torch.linalg.vector_norm(signed_err1)),\n",
    "        (signed_err2.mean(), signed_err2.max(), torch.linalg.vector_norm(signed_err2)),\n",
    "    ]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b2fc95-eac6-493a-9cde-231ce9ffc95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "acts = record_activations_by_dtype(\"google/gemma-2-2b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442dced0-e59c-4f64-a9af-7067e5fb6337",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in acts[\"base\"].keys():\n",
    "    print(f\"[{key}]\")\n",
    "    base_acts = acts[\"base\"][key]\n",
    "    f32s_acts = acts[\"f32s\"][key]\n",
    "    \n",
    "    for dtypes in [\"f16s\", \"bf16s\"]:\n",
    "        print(f\"  [{dtypes}]\")\n",
    "        quant_acts = acts[dtypes][key]\n",
    "        for i in range(len(quant_acts)):\n",
    "            err_info = compare_activations(base_acts[i], f32s_acts[i], quant_acts[i])\n",
    "            print(f\"    {err_info[0]}\")\n",
    "            print(f\"    {err_info[1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
