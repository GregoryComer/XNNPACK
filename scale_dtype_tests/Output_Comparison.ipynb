{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4530ad82-5d87-4565-b5a5-96ca0a917ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "from torchao.quantization.quant_api import (\n",
    "    quantize_,\n",
    "    int8_dynamic_activation_int4_weight,\n",
    ")\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37567f33-e53e-49f1-9635-186db0352bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quant_with_scale_dtype(model, scale_dtype, group_size):\n",
    "    quantize_(model, int8_dynamic_activation_int4_weight(group_size=group_size))\n",
    "\n",
    "    if scale_dtype != torch.float32:\n",
    "        for name, m in model.named_modules():\n",
    "            if isinstance(m, torch.nn.Linear):\n",
    "                original_weight_tensor = m.weight.data.original_weight_tensor\n",
    "                new_scales = original_weight_tensor.layout_tensor.scale.to(scale_dtype).to(torch.float32)\n",
    "                original_weight_tensor.layout_tensor.scale.copy_(new_scales)\n",
    "\n",
    "    return model\n",
    "\n",
    "def generate_with_scale_dtype(model_id, prompts, scale_dtype, group_size=256, max_new_tokens=100):\n",
    "    generation_kwargs = {\n",
    "        \"do_sample\": False,\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "    }\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"cuda\",\n",
    "        torch_dtype=torch.float32,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    model = quant_with_scale_dtype(model, scale_dtype, group_size)\n",
    "\n",
    "    base_pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "    return [base_pipe(p, **generation_kwargs)[0][\"generated_text\"] for p in prompts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5049d2a5-1223-43e8-b879-53b6c8dd8b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale DType: torch.float32\n",
      "Scale DType: torch.float16\n",
      "Scale DType: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "responses = {}\n",
    "prompts = [\"Once upon a time,\", \"A script to print 1 through 10 in python is: \", \"Q: What is the difference between integration and differentiation? A:\", \"Q: How does gradient descent work? A:\"]\n",
    "\n",
    "for dtype in [torch.float32, torch.float16, torch.bfloat16]:\n",
    "    print(f\"Scale DType: {dtype}\")\n",
    "    text = generate_with_scale_dtype(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", prompts, dtype, group_size=32)\n",
    "    responses[dtype] = text\n",
    "\n",
    "for f32s_text, f16s_text, bf16s_text in zip(responses[torch.float32], responses[torch.float16], responses[torch.bfloat16]):\n",
    "    print(f\" f32: {f32s_text}\")\n",
    "    print(\" - - - - - - \")\n",
    "    print(f\" f16: {f16s_text}\")\n",
    "    print(\" - - - - - - \")\n",
    "    print(f\" bf16: {bf16s_text}\")\n",
    "    print()\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
